{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indexing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDd6YEeO64_",
        "outputId": "88dfe85e-d228-4278-f96b-13640bc96c6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXXSwSkUa8xK"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVcwr_mtO7ou"
      },
      "source": [
        "import re, os, sys, nltk\n",
        "import xml.etree.cElementTree as et\n",
        "import pickle, base64, time\n",
        "from heapq import *\n",
        "import math, operator, json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVU1Pn7K7ex-"
      },
      "source": [
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "stop_words = {}\n",
        "stop_file = open(\"/content/drive/My Drive/Wiki-Search/stop_words.txt\", \"r\")\n",
        "words = stop_file.read()\n",
        "words = words.split(\",\")\n",
        "for word in words:\n",
        "    word = word.strip()\n",
        "    if word:\n",
        "        stop_words[word[1:-1]] = 1\n",
        "#print(stop_words)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAdzPuoZdTuA"
      },
      "source": [
        "pattern = re.compile(\"[^a-zA-Z0-9]\")\n",
        "cssExp = re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
        "linkExp = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6P8Wv6hrlUi"
      },
      "source": [
        "##XML Parser for reading XML files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btBVCEEEPEQD"
      },
      "source": [
        "# arguments = sys.argv\n",
        "#wikipedia_dump = sys.argv[1]\n",
        "index_path = \"/content/drive/My Drive/Wiki-Search/\"\n",
        "wikipedia_dump = \"/content/drive/My Drive/Wiki-Search/Input-Data/input.xml\"\n",
        "content = et.iterparse(wikipedia_dump, events=(\"start\", \"end\"))\n",
        "content = iter(content)\n",
        "# document_title = open(\"index/document_title.pickle\", \"wb\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWfR-1V0gIMi"
      },
      "source": [
        "title_inverted_index = {}\n",
        "body_inverted_index = {}\n",
        "category_inverted_index = {}\n",
        "infobox_inverted_index = {}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CrzUy8TZlAP"
      },
      "source": [
        "fcount, per_page_document = 0, 35000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ym_yV5Xg4lW"
      },
      "source": [
        "document_no = 1\n",
        "title_freq = {}\n",
        "body_freq = {}\n",
        "category_freq = {}\n",
        "infobox_freq = {}\n",
        "document_title = open(\"/content/drive/My Drive/Wiki-Search/titles.txt\",\"w+\")\n",
        "document_title_position = []\n",
        "document_word = {}\n",
        "start = time.time()\n",
        "store_stemmed_word = {}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYcwE8Ejg7PW"
      },
      "source": [
        "def write_into_file(filename,inverted_object,flag):\n",
        "    global document_word\n",
        "    fileptr = open(filename, \"w+\")\n",
        "    pointer = 0\n",
        "    for word in inverted_object:\n",
        "        posting_list = \",\".join(inverted_object[word])\n",
        "        posting_list = posting_list + \"\\n\"\n",
        "        if word not in document_word:\n",
        "            document_word[word] = {}\n",
        "        document_word[word][flag] = pointer\n",
        "        fileptr.write(posting_list)\n",
        "        pointer += len(posting_list)\n",
        "    fileptr.close()\n",
        "\n",
        "def write_pickle_file(filename, pickleobj):\n",
        "    file = open(filename, \"wb\")\n",
        "    pickle.dump(pickleobj, file)\n",
        "    file.close()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEEJW1BZrviu"
      },
      "source": [
        "##Preprocessing(Tokenization,Casefolding,Stopword removal,Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPoKvLzJ1YW9"
      },
      "source": [
        "for event,context in content:\n",
        "    tag = re.sub(r\"{.*}\", \"\", context.tag)\n",
        "    \n",
        "    if event == \"end\":\n",
        "        \n",
        "        if tag == \"title\":\n",
        "            \n",
        "            title_text = context.text\n",
        "            position = document_title.tell()\n",
        "            document_title_position.append(position)\n",
        "            document_title.write(title_text + \"\\n\")\n",
        "            title_text = title_text.lower()\n",
        "            try:\n",
        "                words = re.split(pattern, title_text)\n",
        "                for word in words:\n",
        "                    word = word.lower()\n",
        "                    if word in store_stemmed_word:\n",
        "                        word = store_stemmed_word[word]\n",
        "                    else:\n",
        "                        stem = stemmer.stem(word)\n",
        "                        store_stemmed_word[word] = stem\n",
        "                        word = stem\n",
        "                    if len(word) <= 2:\n",
        "                        continue\n",
        "                    if word and word not in stop_words:\n",
        "                        if word not in title_freq:\n",
        "                            title_freq[word] = 1\n",
        "                        else:\n",
        "                            title_freq[word] += 1\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        elif tag == \"text\":\n",
        "            \n",
        "            body_text = context.text\n",
        "            body_text = linkExp.sub('',str(body_text))\n",
        "            body_text = cssExp.sub('',str(body_text))\n",
        "            try:\n",
        "                category_words = re.findall(\"\\[\\[Category:(.*?)\\]\\]\", body_text);\n",
        "                if category_words != \"\":\n",
        "                    for category_word in category_words:\n",
        "                        words = re.split(pattern, category_word)\n",
        "                        for word in words:\n",
        "                            word = word.lower()\n",
        "                            if word in store_stemmed_word:\n",
        "                                word = store_stemmed_word[word]\n",
        "                            else:\n",
        "                                stem = stemmer.stem(word)\n",
        "                                store_stemmed_word[word] = stem\n",
        "                                word = stem\n",
        "                            if len(word) <= 2:\n",
        "                                continue\n",
        "                            if  word and word not in stop_words:\n",
        "                                if word not in category_freq:\n",
        "                                    category_freq[word] = 1\n",
        "                                else:\n",
        "                                    category_freq[word] += 1\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            try:\n",
        "\n",
        "                info_words = re.findall(\"{{Infobox((.|\\n)*?)}}\", body_text)\n",
        "                if info_words != \"\":\n",
        "                    for info_word in info_words:\n",
        "                        for i_word in info_word:\n",
        "                            words = re.split(pattern, i_word)\n",
        "                            for word in words:\n",
        "                                word = word.lower()\n",
        "                                if word in store_stemmed_word:\n",
        "                                    word = store_stemmed_word[word]\n",
        "                                else:\n",
        "                                    stem = stemmer.stem(word)\n",
        "                                    store_stemmed_word[word] = stem\n",
        "                                    word = stem\n",
        "                                if len(word) <= 2:\n",
        "                                    continue\n",
        "                                if word and word not in stop_words:\n",
        "                                    if word not in infobox_freq:\n",
        "                                        infobox_freq[word] = 1\n",
        "                                    else:\n",
        "                                        infobox_freq[word] += 1\n",
        "            except:\n",
        "                pass\n",
        "                                    \n",
        "            try:\n",
        "                words = re.split(pattern, body_text)\n",
        "\n",
        "                for word in words:\n",
        "                    word = word.lower()\n",
        "                    if word in store_stemmed_word:\n",
        "                        word = store_stemmed_word[word]\n",
        "                    else:\n",
        "                        stem = stemmer.stem(word)\n",
        "                        store_stemmed_word[word] = stem\n",
        "                        word = stem\n",
        "                    if len(word) <= 2:\n",
        "                        continue\n",
        "                    if word and word not in stop_words:\n",
        "                        if word not in body_freq:\n",
        "                            body_freq[word] = 1\n",
        "                        else:\n",
        "                            body_freq[word] += 1\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "        elif tag == \"page\":\n",
        "            d_no = str(document_no)\n",
        "            for word in body_freq:\n",
        "                if word not in body_inverted_index:\n",
        "                    body_inverted_index[word]=[]\n",
        "                body_inverted_index[word].append(d_no + \":\" + str(body_freq[word]))\n",
        "            \n",
        "            body_freq.clear()\n",
        "\n",
        "            for word in title_freq:\n",
        "                if word not in title_inverted_index:\n",
        "                    title_inverted_index[word]=[]\n",
        "                title_inverted_index[word].append(d_no + \":\" + str(title_freq[word]))\n",
        "            \n",
        "            title_freq.clear()\n",
        "\n",
        "            for word in category_freq:\n",
        "                if word not in category_inverted_index:\n",
        "                    category_inverted_index[word]=[]\n",
        "                category_inverted_index[word].append(d_no + \":\" + str(category_freq[word]))\n",
        "            \n",
        "            category_freq.clear()\n",
        "            \n",
        "            for word in infobox_freq:\n",
        "                if word not in infobox_inverted_index:\n",
        "                    infobox_inverted_index[word]=[]\n",
        "                infobox_inverted_index[word].append(d_no + \":\" + str(infobox_freq[word]))\n",
        "                \n",
        "            infobox_freq.clear()\n",
        "            \n",
        "            if document_no%80000:\n",
        "                store_stemmed_word = {}\n",
        "            \n",
        "            if document_no%per_page_document==0:\n",
        "                filename = \"/content/drive/My Drive/Wiki-Search/title_\" + str(fcount) + \".txt\"\n",
        "                write_into_file(filename,title_inverted_index,'t')\n",
        "                title_inverted_index.clear()\n",
        "                \n",
        "                filename = \"/content/drive/My Drive/Wiki-Search/category_\" + str(fcount) + \".txt\"\n",
        "                write_into_file(filename,category_inverted_index,'c')\n",
        "                category_inverted_index.clear()\n",
        "                \n",
        "                filename = \"/content/drive/My Drive/Wiki-Search/infobox_\" + str(fcount) + \".txt\"\n",
        "                write_into_file(filename,infobox_inverted_index,'i')\n",
        "                infobox_inverted_index.clear()\n",
        "                \n",
        "                filename = \"/content/drive/My Drive/Wiki-Search/body_text_\" + str(fcount) + \".txt\"\n",
        "                write_into_file(filename,body_inverted_index,'b')\n",
        "                body_inverted_index.clear()\n",
        "                fcount = fcount + 1\n",
        "            document_no += 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytPJ910RnNSE",
        "outputId": "65f98204-ff3b-4d4d-eb8e-0ebc2d270ee4"
      },
      "source": [
        "print(len(title_inverted_index))\n",
        "print(len(body_inverted_index))\n",
        "print(len(category_inverted_index))\n",
        "print(len(infobox_inverted_index))\n",
        "print(len(infobox_freq))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23304\n",
            "227389\n",
            "11505\n",
            "21767\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zyjv78D1YW-"
      },
      "source": [
        "document_title.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FutnIFlAsVav"
      },
      "source": [
        "##Save output files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEAeYlYw1YW-"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/title_position.pickle\"\n",
        "write_pickle_file(filename,document_title_position)\n",
        "document_title_position.clear()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK4K7YIj1YW-"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/title_\" + str(fcount) + \".txt\"\n",
        "write_into_file(filename,title_inverted_index,'t')\n",
        "title_inverted_index.clear()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD6ho_7D1YW_"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/category_\" + str(fcount) + \".txt\"\n",
        "write_into_file(filename,category_inverted_index,'c')\n",
        "category_inverted_index.clear()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTb-eqI61YW_"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/infobox_\" + str(fcount) + \".txt\"\n",
        "write_into_file(filename,infobox_inverted_index,'i')\n",
        "infobox_inverted_index.clear()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvikPVzZ1YW_"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/body_text_\" + str(fcount) + \".txt\"\n",
        "write_into_file(filename,body_inverted_index,'b')\n",
        "body_inverted_index.clear()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88R0zu131YW_"
      },
      "source": [
        "fcount = fcount + 1\n",
        "type_of_files = [\"/content/drive/My Drive/Wiki-Search/title_\",\"/content/drive/My Drive/Wiki-Search/category_\",\"/content/drive/My Drive/Wiki-Search/infobox_\",\"/content/drive/My Drive/Wiki-Search/body_text_\"]\n",
        "mapping = {}\n",
        "mapping[\"/content/drive/My Drive/Wiki-Search/infobox_\"] = 'i'\n",
        "mapping[\"/content/drive/My Drive/Wiki-Search/body_text_\"] = 'd'\n",
        "mapping[\"/content/drive/My Drive/Wiki-Search/title_\"] = 't'\n",
        "mapping[\"/content/drive/My Drive/Wiki-Search/category_\"] = 'c'\n",
        "output_files = []"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dnxYq2DsdC7"
      },
      "source": [
        "##Inverted Indexing/Posting List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgC2aSff1YXA"
      },
      "source": [
        "for f in type_of_files:\n",
        "    heap = []\n",
        "    input_files = []\n",
        "    output_file = f + \"output.txt\"\n",
        "    output_ptr = open(output_file,'w+')\n",
        "    output_files.append(output_ptr)\n",
        "    output_f_no = len(output_files) - 1\n",
        "    \n",
        "    for file_no in range(fcount):\n",
        "        fname = f + str(file_no) + \".txt\"\n",
        "        fptr = open(fname,\"r\")\n",
        "        if os.stat(fname).st_size == 0:\n",
        "            continue\n",
        "        input_files.append(fptr)\n",
        "    \n",
        "    if len(input_files) == 0:\n",
        "        break\n",
        "    \n",
        "    for file_no in range(fcount):\n",
        "        try:\n",
        "            line = input_files[file_no].readline()[:-1]\n",
        "#             print(line)\n",
        "            heap.append((line,file_no))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    heapify(heap)\n",
        "    \n",
        "    file_no = 0\n",
        "    try:\n",
        "        while file_no < fcount:\n",
        "            line, f_no = heappop(heap)\n",
        "            word = line.split(\"-\")[0]\n",
        "            posting_list = line.split(\"-\")[1]\n",
        "            next_line = input_files[f_no].readline()[: -1]\n",
        "            if next_line!= \"\":\n",
        "                heappush(heap, (next_line, f_no))\n",
        "            else:\n",
        "                file_no = file_no + 1\n",
        "\n",
        "            while file_no < fcount:\n",
        "                try:\n",
        "                    next_line, next_f_no = heappop(heap)\n",
        "                    next_word = next_line.split(\"-\")[0]\n",
        "                    next_posting_list = next_line.split(\"-\")[1]\n",
        "                except IndexError:\n",
        "                    break\n",
        "\n",
        "                if next_word == word:\n",
        "                    posting_list = posting_list + \",\" + next_posting_list\n",
        "                    next_next_line = input_files[next_f_no].readline()[:-1]\n",
        "                    if next_next_line:\n",
        "                        heappush(heap, (next_next_line, next_f_no))\n",
        "                    else:\n",
        "                        file_no = file_no + 1\n",
        "                else:\n",
        "                    heappush(heap, (next_line, next_f_no))\n",
        "                    break\n",
        "\n",
        "            if word not in document_word:\n",
        "                document_word[word]={}\n",
        "\n",
        "            document_word[word][mapping[f]] = output_files[output_f_no].tell()\n",
        "            final_posting_list = posting_list.split(\",\")\n",
        "\n",
        "            idf = math.log10(document_no / len(final_posting_list))\n",
        "            documents_score = {}\n",
        "            for post in final_posting_list:\n",
        "\n",
        "                doc_no = int(post.split(\":\")[0])\n",
        "                word_freq = int(post.split(\":\")[1])\n",
        "                tf = 1 + math.log10(word_freq)\n",
        "                documents_score[doc_no] = round(idf * tf, 2)\n",
        "\n",
        "            score = \"\"\n",
        "            count = 0\n",
        "            for document in documents_score.keys():\n",
        "                score = score + str(document) + \":\" + str(documents_score[document]) + \",\"\n",
        "                count = count + 1\n",
        "            score = score[:-1] + \"\\n\"\n",
        "    #             print(score)\n",
        "            output_files[output_f_no].write(score)\n",
        "    except IndexError:\n",
        "        pass\n",
        "    \n",
        "    output_files[output_f_no].close()\n",
        "    \n",
        "    for f_no in range(fcount):\n",
        "        file_ = f + str(f_no) + \".txt\"\n",
        "        os.remove(file_)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5DSCHNa1YXA"
      },
      "source": [
        "filename = \"/content/drive/My Drive/Wiki-Search/word_position.json\"\n",
        "with open(filename,\"w+\") as f:\n",
        "    json.dump(document_word,f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mLA5RsV1YXA",
        "outputId": "c124413e-adda-4134-d358-88080ce39da2"
      },
      "source": [
        "end = time.time()\n",
        "print(\"Time Taken :- \",(end-start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Taken :-  80.38945889472961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg9qOYU4J94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}